# 强化学习与LLM

OpenAI 所做的报告《Reinforcement Learning from Human Feedback: Progress and Challenges》分享了强化学习在大语 言模型上的重要作用，可以概括为以下几个方面。

**强化学习比有监督学习更可以考虑整体影响**：有监督学习针对单个词元进行反馈，其目 标是要求模型针对给定的输入给出的确切答案。而强化学习是针对整个输出文本进行反馈，并不 针对特定的词元。这种反馈粒度的不同，使得强化学习更适合大语言模型，既可以兼顾表达多样 性，还可以增强对微小变化的敏感性。

自然语言十分灵活，可以用多种不同的方式表达相同的语 义。而有监督学习很难支持上述学习方式。强化学习则可以允许模型给出不同的多样性表达。

有监督微调通常采用交叉熵损失做为损失函数，由于总和规则，造成这种损失对个别 词元变化不敏感，如果改变个别的词元，只会对整体损失产生小的影响。但是，一个否定词可以 完全改变文本的整体含义。强化学习则可以通过奖励函数达到同时兼顾多样性和微小变化敏感性 两个方面。

**强化学习更容易解决幻觉问题**：用户在大语言模型时主要有三类输入：

- 文本型（TextGrounded）：用户输入相关文本和问题，让模型基于所提供的文本生成答案（例如，“本文中提到 的人名和地名有哪些”）；
- 求知型（Knowledge-Seeking）：用户仅提出问题，模型根据内在知识提供真实回答（例如，“流感的常见原因是什么”）；
- 创造型（Creative）：用户为提供问题或 说明，让模型进行创造性输出（例如，“写一个关于... 的故事”）。

有监督学习算法非常容易使得求 知型查询产生幻觉。在模型并不包含或者知道答案的情况下，有监督训练仍然会促使模型给出答案。而使用强化学习方法，则可以通过定制奖励函数，将正确答案赋予非常高的分数，放弃回答 的答案赋予中低分数，不正确的答案赋予非常高的负分，使得模型学会依赖内部知识选择放弃回 答，从而在一定程度上缓解模型幻觉问题。

**强化学习可以更好的解决多轮对话奖励累积问题**：多轮对话能力是大语言模型重要的基 础能力之一，多轮对话是否达成最终目标，需要考虑多次交互过程的整体情况，因此很难使用有 监督学习方法构建。而使用强化学习方法，可以通过构建奖励函数，将当前输出考虑整个对话的 背景和连贯性。

## PPO

LLM 的预训练要用到大量来自不同来源的语料库，而这本身就无法确保这些数据集的质量。此外，LLM 的主要目标是预测下一个 token，这个目标与「有用且安全地遵从用户指令」的目标并不一致。因此，LLM 可能会输出不真实、有害或对用户无用的内容。本质上讲，这些模型并未与用户意图对齐。**RLHF/PPO 的主要目标是在各种任务上对齐语言模型与用户意图**，**其做法是使用人类反馈来微调模型**。

> 以下部分待完善，没搞过强化学习

PPO（Proximal Policy Optimization）是OpenAI在2017提出的一种强化学习算法，是基于策略优化的算法，用于训练能够最大化累积奖励的智能体。PPO算法通过在每次更新时限制新策略与旧策略之间的差异，从而更稳定地更新策略参数。这种方法有助于避免训练过程中出现的不稳定性和剧烈波动，使得算法更容易收敛并学习到更好的策略。

PPO的想法是，通过限制在每个训练阶段对策略所做的更改来提高策略的训练稳定性：我们希望避免过大的策略(参数)更新。我们从经验上知道，训练期间较小的策略更新更有可能收敛到最优解。策略更新步幅太大可能导致“坠崖”（获得糟糕的策略），并且需要很长时间甚至不可能恢复。

因此，使用PPO，我们要保守地更新策略。为此，我们需要通过计算当前策略与前一个策略之间的比率来衡量当前策略与前一个策略相比发生了多大的变化。我们将此比率限制在 [1−ϵ,1+ϵ] 的范围内，这意味着我们消除了当前策略偏离旧策略太远的动机（因此称为近端策略术语）。

## DPO

DPO（Direct Preference Optimization， 直接偏好优化）通过利用奖励函数与最优策略之间的映射关系，证明这个受限的奖励最大化问题可以通过单阶段的策略训练来精确优化，本质上是在人类偏好数据上解决一个分类问题。

它的工作原理是创建人类偏好对的数据集，每个偏好对都包含一个提示和两种可能的完成方式——一种是首选，一种是不受欢迎。然后对LLM进行微调，以最大限度地提高生成首选完成的可能性，并最大限度地减少生成不受欢迎的完成的可能性。

DPO是稳定的、性能和计算成本轻量级的，无需拟合奖励模型，在微调期间从 LM 中采样，或执行显着的超参数调整。通过实验表明：DPO 进行微调超过了 RLHF 效果，并提高了摘要和单轮对话的响应质量。

传统的RLHF(奖励模型+PPO)步骤为：

1. 预训练的基础LLM
2. 监督微调（SFT）LLM
3. 奖励模型（LLM，但修改为奖励模型）
4. PPO优化的语言模型（最终与偏好对齐的LLM）

DPO通过完全移除奖励模型来简化这个过程：

1. 预训练的基础LLM
2. 监督微调（SFT）LLM
3. DPO优化的语言模型（最终与偏好对齐的LLM）