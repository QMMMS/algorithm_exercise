# 统计学习引入

> 参考：https://blog.princehonest.com/stat-learning/

## 0 基本问题

- 什么情况下需要进⾏估计f？
- Y作为预测，其精确度依赖于哪些量？
- 如何区分推断和预测？
- 推断中常⽤的基本问题有哪些？
- 如何利⽤均⽅误差计算可约误差和不可约误差？
- 估计f的⽅法有哪些？ 
- 半指导学习的适⽤的数据模型为哪些？
- 模型的拟合效果如何评价（针对回归类模型）？
- 描述曲线光滑度的量是什么？
- 光滑度和偏差、⽅差的关系？
- 分类模型最常⽤的估计精度的⽅法是什么？
- ⻉叶斯错误率如何计算？ 
- KNN算法的实现步骤？
- 光滑度⾼的模型优缺点是什么？适⽤的情况是什么？
- KNN算法中当K逐渐增⼤，边界将如何变化？

## 1 统计学习历史人物

- 托马斯·贝叶斯，18世纪英国数学家，提出贝叶斯定理。
- 高斯提出正态分布。
- 19世纪初，勒让德，高斯在发表有关最小二乘法的文章时，提出了线性回归的最早形式。 
- 1936年费舍尔提出了线性判断分析，之后又有人提出了逻辑斯蒂回归。 
- 70年代，内尔德和韦德伯恩提出了广义线性回归。
- 20 世纪 80 年代，计算技术条件具备，非线性模型不再受计算的困扰。 
- 80 年代中期，布赖曼( Breiman) 、弗里德曼( Friedman) 、奥申 ( Olshen) 和斯通 (Stone) 提出了分类回归树，以及交叉验证法。 
- 1986 年，哈斯帖( Hastie) 和提布施瓦尼( Tibshirani )提出广义可加模型，开发了实用软件实现模型。
- 卡尔·皮尔逊提出了统计假设检验，相关系数， 卡方检验，P值。
- 威廉·希利·戈塞提出统计学t检验。
- 罗纳德·费雪提出方差分析、Fisher精确检验。
- C R 拉奥提出C-R不等式、Rao-Blackwell定理、微分几何和统计。
- 弗罗伦斯·南丁格尔，英国护士，提出南丁格尔玫瑰图。

## 2.1 什么是统计学习

**输入变量**通常用 **X** 表示，也称为**预测变量**、**自变量**、**属性变量**

**输出变量**通常用 **Y** 表示，也称为**响应变量**、**因变量**
$$
Y = f(X) + ε
$$
f是X的函数，固定但未知，f表达了X提供给Y的系统信息，ε是**随机误差项**（与X独立，均值为0）

### 2.1.1 什么情况下需要估计f

**预测**和**推断**。

- 预测主要关心f的估计值的准确性，不关注其是如何预测的（将f当作黑箱）精确性包括**可约误差**与**不可约误差**，可约误差可以通过选择更合适的统计学习方法降低
- 推断主要关心$X_1$,$X_2$,...变化时如何对Y产生影响（不能将f当作黑箱）

### 2.1.2 如何估计f

估计f的⽅法有哪些？估计任务大多可分为**参数方法**和**非参数方法**

- 参数方法假设函数f具有一定的形式，用训练数据集去拟合模型（估计参数），即把估计f的问题简化到估计一组参数
- 非参数方法不对函数f的形式做明确的假设，追求尽可能接近数据点，例如薄板样条

> 过拟合：拟合了错误或噪声

### 2.1.3 预测精度和模型解释性的权衡

一般来说，当一种方法的光滑度增强时，其解释性减弱

### 2.1.4 指导学习和无指导学习

- 指导学习：数据集中有对应的响应变量来指导数据分析，例如逻辑斯蒂回归、支持向量机
- 无指导学习：数据集缺乏一个响应变量来指导数据分析，例如聚类分析
- 半指导学习：部分有，部分没有

### 2.1.5 回归与分类问题

变量常分为**定量**和**定性**两种类型。响应变量定量是回归问题，响应变量定性则是分类问题

## 2.2 评价模型精度

### 2.2.1 拟合效果检验

常用的评价准则是**均方误差**（mean squared error, **MSE**）
$$
MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2
$$
其中，n是观测个数。

我们的目标是使模型的**测试均方误差**最低。

当模型的光滑度增加时，训练均方误差降低，但是测试均方误差不一定降低

> 当模型有较小的训练均方误差，但是有较大的测试均方误差时，称为过拟合
>
> 降低模型的光滑度可以减小测试均方误差

自由度是描述曲线光滑程度的正式术语

### 2.2.2 偏差-方差权衡

$$
E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(ε)
$$

> 期望平方误差值能分解为：预测值$\hat{f}(x_0)$的方差、预测值$\hat{f}(x_0)$的偏差的平方、误差项ε的方差

因此我们需要得到一个偏差和方差综合起来最小的模型

- 偏差（bias）指的是为了选择一个简单的模型逼近真实函数而被带入的误差
- 方差（variance）代表的是用一个不同的训练数据集估计f时，估计函数的改变量

![](img/2_1.png)

如果一个统计学习模型被称为测试性能好，则要求该模型有较小的方差和较小的偏差

| 模型             | 偏差减小，方差增大 | 方差减小，偏差增大 |
| ---------------- | ------------------ | ------------------ |
| 线性回归         | 系数个数增多       | 系数个数减少       |
| K最近邻（KNN）   | K减小              | K增大              |
| **岭回归/Lasso** | λ减小              | λ增大              |
| 多项式回归       | 最高项次数增大     | 最高项次数减小     |
| 阶梯函数         | 分割点个数增多     | 分割点个数减少     |
| 回归样条         | 自由度增大         | 自由度减小         |
| **光滑样条**     | λ减小              | λ增大              |
| 局部回归         | 比例s减小          | 比例s增大          |
| 广义可加模型     | --                 | --                 |
| **决策树**       | α减小              | α增大              |
| 支持向量分类器   | C减小/cost值增大   | C增大/cost值减小   |

### 2.2.3 分类模型

常用的评价准则是**错误率**，定义为：
$$
\frac{1}{n}\sum_{i=1}^n I(y_i \neq \hat{y_i})
$$

- 其中，n是观测个数。
- $$I(y_i \neq \hat{y_i})$$是一个示性变量，当$$y_i \neq \hat{y_i}$$，值为1，否则为0

常用的方法是贝叶斯分类器、K最近邻方法。

贝叶斯分类器中，观测$x_0$会被分配到使下面式子值最大的 j 类：
$$
\text{Pr}(Y=j|X=x_0)
$$
在图上，使用贝叶斯分类器画出的分割线称为**贝叶斯决策边界**。

贝叶斯分类器将产生的最低的测试错误率被称为**贝叶斯错误率**，类似于不可约误差。一般来说，贝叶斯错误率是：
$$
1-E(\max_j \text{Pr}(Y=j|X))
$$
在K最近邻方法中，贝叶斯规则为：
$$
\text{Pr}(Y=j|X=x_0)=\frac{1}{K}\sum_{i \in \mathcal{N}_0}I(y_i=j)
$$
其中，$$\mathcal{N}_0$$表示最接近 $$x_0$$ 的 K 个点的集合。

> 说人话，就是看 $x_0$ 旁边的点的类别，近朱者赤。

当 K 增加时，模型的线性程度增强。