# 浅层神经网络(Shallow neural networks)

## 神经网络结构

![一个简单神经的网络结构](./img/L1_week3_3.png)

许多sigmoid单元堆叠起来形成一个神经网络。

1. 输入特征$x_1$、$x_2$、$x_3$，它们被竖直地堆叠起来，这叫做神经网络的**输入层**。它包含了神经网络的输入
2. 另外一层我们称之为**隐藏层**（中间四个结点）。这些中间结点的准确值我们是不知道到的
3. 最后一层只由一个结点构成，而这个只有一个结点的层被称为**输出层**，它负责产生预测值

## 正向传播

使用上标符号$^{[m]}$表示第$m$层网络中节点相关的数，这些节点的集合被称为第$m$层网络。

>  复习一下：上标符号$^{(i)}$表示第$i$个训练样本

在约定俗成的符号传统中，对于这个例子，只能叫做一个两层的神经网络。原因是输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。输入层称为第零层。

对于隐藏层，第一个单元或结点我们将其表示为$a^{[1]}_{1}$，第二个结点的值我们记为$a^{[1]}_{2}$，以此类推，所以这里的是一个四维的列向量。
$$
a^{[1]} =
	\left[
		\begin{matrix}
		a^{[1]}_{1}\\
		a^{[1]}_{2}\\
		a^{[1]}_{3}\\
		a^{[1]}_{4}
		\end{matrix}
	\right]
$$
### 一个样本的单结点计算

对于$a^{[1]}_{1}$节点，把上一层（一个输入样本）的所有输出作为输入进行计算，并且有对应的参数 $w^{[1]}_1$ （一个列向量，维度为：输入数量 * 1 ）和参数 $b^{[1]}_1$ （一个数），根据之前学习的逻辑回归，有公式：
$$
z^{[1]}_{1}=w^{[1]T}_1x+b^{[1]}_1
$$

$$
a^{[1]}_{1}=\sigma (z^{[1]}_{1})
$$

### 一个样本的一层结点计算

现在把同一层的结点组合起来，有公式：
$$
\left[
    \begin{array}{}
    a^{[1]}_{1}\\
    a^{[1]}_{2}\\
    a^{[1]}_{3}\\
    a^{[1]}_{4}
    \end{array}
\right]
= 
\left[
    \begin{array}{}
    \sigma(z^{[1]}_{1})\\
    \sigma(z^{[1]}_{2})\\
    \sigma(z^{[1]}_{3})\\
    \sigma(z^{[1]}_{4})
    \end{array}
\right]
$$

$$
\left[
		\begin{array}{c}
		z^{[1]}_{1}\\
		z^{[1]}_{2}\\
		z^{[1]}_{3}\\
		z^{[1]}_{4}\\
		\end{array}
		\right]
		 =
	\overbrace{
	\left[
		\begin{array}{c}
		...w^{[1]T}_{1}...\\
		...w^{[1]T}_{2}...\\
		...w^{[1]T}_{3}...\\
		...w^{[1]T}_{4}...
		\end{array}
		\right]
		}^{W^{[1]}}
		*
	\overbrace{
	\left[
		\begin{array}{c}
		x_1\\
		x_2\\
		x_3\\
		\end{array}
		\right]
		}^{input}
		+
	\overbrace{
	\left[
		\begin{array}{c}
		b^{[1]}_1\\
		b^{[1]}_2\\
		b^{[1]}_3\\
		b^{[1]}_4\\
		\end{array}
		\right]
		}^{b^{[1]}}
$$

简洁一点来写：
$$
a^{[1]}=\sigma(z^{[1]})
$$

$$
z^{[1]}=W^{[1]}x+b^{[1]}
$$

- 参数 $W^{[1]}$ 是一个维度为（本层结点数 ， 输入数量）的矩阵
- $z^{[1]}$，$a^{[1]}$，参数 $b^{[1]}$ 是一个维度为（本层结点数 ，1）的列向量
- $x$ 为输入，是一个维度为（输入数量，1）的列向量

同样的，对于输出层：
$$
\hat{y}=a^{[2]}=\sigma(z^{[2]})
$$

$$
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}
$$

### 全部样本的一层结点计算

现在把所有样本组合起来，符号定义和维度为：
$$
\underbrace{X}_{(特征数量，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

$$
\underbrace{Z^{[1]}}_{(本层结点数，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

$$
\underbrace{A^{[1]}}_{(本层结点数，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		\alpha^{[1](1)} & \alpha^{[1](2)} & \cdots & \alpha^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

$$
\underbrace{B^{[1]}}_{(本层结点数，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		b^{[1](1)} & b^{[1](2)} & \cdots & b^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

> 事实上，对于全部样本的一次计算，各层的参数 $W$ 和参数 $b$ 是不变的，全部样本计算完才计算梯度
>
> 也就是说，$b^{[1](1)}=b^{[1](2)}=\cdots  b^{[1](m)}$
>
> 所以矩阵 $B^{[1]}$ 可以用列向量 $b^{[1]}$ 简化表示，python在计算时会自动使用**广播**技术填充

> 可以这么理解：对于$X,Z,A,B$这些矩阵的任意一个，矩阵内部左右移动为样本的选择，上下移动为一层结点的选择

于是有公式：
$$
A^{[1]} = \sigma(Z^{[1]})
$$

$$
Z^{[1]} = W^{[1]}X + B^{[1]}=W^{[1]}A^{[0]} + B^{[1]}
$$

$$
A^{[2]}=\sigma(Z^{[2]})
$$

$$
Z^{[2]}=W^{[2]}A^{[1]}+B^{[2]}
$$

## 激活函数

------

![](./img/jh.png)

![](./img/jh2.png)

到目前为止，只用过**sigmoid**激活函数，但是，有时其他的激活函数效果会更好。这里介绍并比较4种

> Q : 为什么需要非线性的激活函数？换句话说是否可以去掉激活函数这一步？
>
> A : 非线性的激活函数是必要的。否则不管神经网络有多少层，都会训练出线型的函数。换句话说，我们希望我们的神经网络处理复杂的任务，这个函数不复杂不行，对吧？

### sigmoid函数

除了输出层是一个二分类问题（$\hat{y}$ 的数值介于0和1之间）基本不会用它，性能比较差。
$$
a=g(z)=\frac{1}{1 + e^{-z}}
$$

$$
\frac{d}{dz}g(z) = {\frac{1}{1 + e^{-z}} (1-\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))
$$

- sigmoid神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0，导致梯度消失。为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。
- *Sigmoid函数的输出不是零中心的*。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。

### tanh函数

$$
g(z)= tanh(z) = \frac{e^{z} - e^{- z}}{e^{z} + e^{- z}}
$$

$$
\frac{d}{{d}z}g(z) = 1 - (tanh(z))^{2}
$$

事实上，**tanh**函数是**sigmoid**的向下平移和伸缩后的结果。**tanh**函数在所有场合都优于**sigmoid**函数，因为其输出的平均值接近于零，因此它可以更好地将数据集中到下一层。

**sigmoid**函数和**tanh**函数两者共同的缺点是，在 $z$ 特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。

### Relu函数

**Relu**是最常用的激活函数，如果不确定用哪个激活函数，就使用**ReLu**。
$$
g(x) =max( 0,z)
$$

$$
g(z)^{'}=
  \begin{cases}
  0&	\text{if z < 0}\\
  1&	\text{if z > 0}\\
undefined&	\text{if z = 0}
\end{cases}
$$

只要$z$是正值的情况下，导数恒等于1，当$z$是负值的时候，导数恒等于0。

- 相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（ [Krizhevsky ](https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/~fritz/absps/imagenet.pdf)等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。
- sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。
- **缺点**：在训练的时候，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的*死亡*，因为这导致了数据多样化的丢失。

### Leaky ReLu函数

Leaky ReLU是为解决“ReLU死亡”问题的尝试。
$$
a = max( 0.01z,z)
$$

$$
g(z)^{'}=
\begin{cases}
0.01& 	\text{if z < 0}\\
1&	\text{if z > 0}\\
undefined&	\text{if z = 0}
\end{cases}
$$

常数不一定是0.01，可以为学习算法选择不同的参数。

最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。

## 神经网络计算流程

------

1. 初始化参数 $W^{[1]}, W^{[2]},b^{[1]},b^{[2]}$
2. 对所有样本，由参数 $W^{[1]}, W^{[2]},b^{[1]},b^{[2]}$ 计算预测值 $\hat{y}$
3. 计算损失 $J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = {\frac{1}{m}}\sum_{i=1}^mL(\hat{y}, y)$
4. 计算导数 $dW^{[1]}, dW^{[2]},db^{[1]},db^{[2]}$ ，并更新参数
5. 回到步骤 2 进行下一轮计算，直到模型可被接受

> Q1：为什么在初始化时要将参数设置为随机数？不能全为0吗？
>
> A1：如果全设置为 0 ，同一层的各个节点参数就是一样的，并且在之后的训练中，同一层的各个节点参数仍然保持一致，这是我们不愿意看到的情况。对于笔记开始的那个神经网络来讲，可以这么写：
> $$
> W^{[1]} = np.random.randn(2,2)\;*\;0.01\;,\;b^{[1]} = np.zeros((2,1))
> $$
>
> $$
> W^{[2]} = np.random.randn(2,2)\;*\;0.01\;,\;b^{[2]} = 0
> $$
>
> Q2：为什么要乘 0.01？
>
> A2：对于sigmoid函数和tanh函数，如果参数 $W$ 过大，激活函数的输入也非常大，从而导致梯度接近于零，换句话说就是“学得慢”。所以我们希望参数 $W$ 能接近 0

## 反向传播与导数计算

------

$$
dZ^{[2]}=A^{[2]}-Y
$$

$$
dW^{[2]}={\frac{1}{m}}dZ^{[2]}{A^{[1]}}^{T}
$$

$$
db^{[2]} = {\frac{1}{m}}\sum dZ^{[2]}
$$

对应代码：`db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)`

> 这里`np.sum`是python的numpy命令，`axis=1`表示水平相加求和，`keepdims`是防止python输出那些古怪的秩数$(n,)$，加上这个确保阵矩阵 $db^{[2]}$ 这个向量输出的维度为$(n,1)$这样标准的形式。 
>
> 还有一种防止python输出奇怪的秩数，需要显式地调用`reshape`把`np.sum`输出结果写成矩阵形式。

$$
\underbrace{dZ^{[1]}}_{(n^{[1]}, m)} = \underbrace{W^{[2]T}dZ^{[2]}}_{(n^{[1]}, m)}*\underbrace{g[1]^{'}(Z^{[1]})}_{(n^{[1]}, m)}
$$

> $*$ 代表进行逐个元素乘积

$$
dW^{[1]} = {\frac{1}{m}}dZ^{[1]}X^{T}
$$


$$
db^{[1]} = {\frac{1}{m}}\sum dZ^{[1]}
$$

对应代码：`db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)`

### 反向传播与计算图

利用计算图和链式法则，我们可以一步一步计算复杂函数的导数。

![](./img/bbe1.png)

- **加法门单元**把输出的梯度相等地分发给它所有的输入
- **取最大值门单元**对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。
- **乘法门单元**相互交换梯度

例子2：对于如下式子
$$
f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}
$$
![](./img/lz.png)

从最右边开始，假设误差为1.00

对于 $f(x)=\frac{1}{x}$，$\frac{df}{dx}=-\frac{1}{x^2}$，传递的误差为$-\frac{1}{1.37^2} \times 1.00=-0.53$

对于 $f(x)=c+x$，$\frac{df}{dx}=1$，传递的误差为$1 \times -0.53=-0.53$

对于 $f(x)=e^x$，$\frac{df}{dx}=e^x$，传递的误差为$e^{-1.00} \times -0.53=-0.20$

对于 $f(x)=-x$，$\frac{df}{dx}=-1$，传递的误差为$-1 \times -0.20=0.20$

对于 $f(x)=x+x'$，$\frac{df}{dx}=1$，传递的误差为$1 \times 0.20=0.20$

对于 $f(x)=x\cdot x'$，$\frac{\partial f}{\partial x}=x'$，对于$w_0$，传递的误差为$-1.00 \times 0.20=-0.20$，对于$x_0$，传递的误差为$2.00 \times 0.20=0.40$，对于$w_1$，传递的误差为$-2.00 \times 0.20=-0.40$，对于$x_1$，传递的误差为$-3.00 \times 0.20=-0.60$

例子3：

![](./img/lz2.png)
$$
\begin{align}
&In_{h_1}=w_1*x_1+w_3*x_2 \\
&h1 = Out_{h_1}=Sigmoid(In_{h_1}) \\
&In_{h_2}=w_2*x_1+w_4*x_2 \\
&h2 = Out_{h_2}=Sigmoid(In_{h_2}) \\
&In_{o_1}=w_5*h_1+w_7*h_2\\
&o_1=Out_{o_1}=Sigmoid(In_{o_1})\\
&In_{o_2}=w_6*h_1+w_8*h_2\\
&o_2=Out_{o_2}=Sigmoid(In_{o_2})\\
&Error=\frac{1}{2}\sum_{i=1}^2(o_i-y_i)^2
\end{align}
$$
如果要计算w5的梯度：
$$
\begin{align}
\delta_5= \frac{\partial Error}{\partial o_1} *\frac{\partial o_1}{\partial In_{o_1}} * \frac{\partial In_{o_1}}{\partial w_5} \\
\frac{\partial Error}{\partial o_1} = o_1-y_1 \\
\frac{\partial o_1}{\partial In_{o_1}}= o_1 *(1-o_1)\\
\frac{\partial In_{o_1}}{\partial w_5} = h_1
\end{align}
$$
如果要计算w1的梯度：
$$
\begin{align}
\delta_5= &\frac{\partial Error}{\partial o_1} *\frac{\partial o_1}{\partial In_{o_1}} * \frac{\partial In_{o_1}}{\partial h_1} *\frac{\partial h_1}{\partial In_{h_1}}*\frac{\partial In_{h_1}}{\partial w_1}+\\
&\frac{\partial Error}{\partial o_2} *\frac{\partial o_2}{\partial In_{o_2}} * \frac{\partial In_{o_2}}{\partial h_1} *\frac{\partial h_1}{\partial In_{h_1}}*\frac{\partial In_{h_1}}{\partial w_1}

\end{align}
$$
更新参数公式：
$$
w_i'=w_i-\eta * \delta_i
$$

### 向量与矩阵求导

当我们想要向量化公式，在求导时，注意向量对向量求导得到的是雅可比矩阵。

![](./img/vtv.png)

当我们使用向量代表x,y,z时，注意各个偏导的维数。

![](./img/vb.png)

当我们使用矩阵代表x,y,z时

![](./img/mtm.png)

