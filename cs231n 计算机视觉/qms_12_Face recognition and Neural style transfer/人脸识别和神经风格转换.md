# 人脸验证

## 任务描述

------

在人脸识别的相关文献中，人们经常提到人脸验证（**face verification**）和人脸识别（**face recognition**）。

-   人脸验证问题的任务是，有一张输入图片，以及某人的**ID**或者是名字，这个系统要做的是，验证输入图片是否是这个人。即回答“是不是这个人？”
-   而人脸识别问题比人脸验证问题难很多，它要回答“这个人是谁？“

这个章节主要讲构造一个人脸验证作为基本模块，如果准确率够高，你也许以把它用在识别系统上。

## Similarity函数

------

我们之前使用**softmax**来输出标签，每个人代表一个类别，但是在人脸识别任务中，我们回答“是不是这个人？”这个问题，我们对最后一层做改变，定义Similarity函数，它是一个用$d$表示的函数：
$$
d(img1,img2) = degree\ of\ difference\ between\ images
$$
它以两张图片作为输入，然后输出这两张图片的差异值。

-   如果放进同一个人的两张照片，希望它能输出一个很小的值。
-   如果放进两个长相差别很大的人的照片，它就输出一个很大的值。
-   如果这两张图片的差异值小于某个阈值$\tau$（一个超参数），那么这时就能预测这两张图片是同一个人。

## Siamese 网络

-----

![](http://www.ai-start.com/dl2017/images/ecd4f7ca6487b4ccb19c1f5039e9d876.png)

如上图，对于一个一般的卷积网络，输入图片$x^{(1)}$，然后通过一些列卷积，池化和全连接层，最终得到特征向量（这里没有**softmax**单元来做分类）。定义$f(x^{(1)})$是输入图像$x^{(1)}$的编码，类似的，$f(x^{(2)})$是输入图像$x^{(2)}$的编码。

编码很好地代表了这两个图片，要做的就是定义$d$函数，将$x^{(1)}$和$x^{(2)}$的距离定义为这两幅图片的编码之差的范数：
$$
d( x^{( 1)},x^{( 2)}) =|| f( x^{( 1)}) - f( x^{( 2)})||_{2}^{2}
$$

## Triplet 损失

------

为了应用梯度下降，需要损失函数，这里使用三元组损失（Triplet 损失）函数。

用三元组损失的术语来说，一次需要准备三张图片：

![](http://www.ai-start.com/dl2017/images/d56e1c92b45d8b9e76c1592fdbf0fc7f.png)

-   一个 **Anchor** 图片，简写成$A$。
-   一个**Positive**图片，**Positive**意味着与**Anchor**是同一个人，简写为$P$。
-   一个**Negative**图片，**Negative**意味着与**Anchor**不是同一个人，简写为$N$。

我们希望对于$f$，**Anchor** 图片与**Positive**图片距离更近，即：
$$
|| f(A) - f(P)||^{2} -||f(A) - f(N)||^{2} +\alpha \leq0
$$
其中$\alpha$是一个正的超参数，也叫做间隔(**margin**)。

然后我们定义损失函数：
$$
L( A,P,N) = max(|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + a,0)
$$
这个$max$函数的作用就是，只要这个$|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + a\leq0$，那么损失函数就是0。网络不会关心它负值有多大。

## 转化为二分类问题

-------

处理人脸验证的另外一种思路是使用之前的二分类，因为对于问题“是不是这个人？”，我们的回答分为“是”或“不是”两种。目标标签是1表示一对图片是一个人，目标标签是0表示图片中是不同的人。

![](http://www.ai-start.com/dl2017/images/c3bf61934da2f20a7d15e183c1d1d2ab.png)

如上图，我们同时看两张照片，得到$f(x^{(1)})$，$f(x^{(2)})$，逻辑回归单元处理的公式为：
$$
\hat y = \sigma(\sum_{k = 1}^{128}{w_{i}| f( x^{( i)})_{k} - f( x^{( j)})_{k}| + b})
$$
其中，下标$k$代表选择这个向量中的第$k$个元素。

还有其他不同的形式来公式，比如$\chi^{2}$公式，也被称为$\chi$平方相似度：
$$
\hat y = \sigma(\sum_{k = 1}^{128}{w_{i}\frac{(f( x^{( i)})_{k} - f(x^{( j)})_{k})^{2}}{f(x^{( i)})_{k} + f( x^{( j)})_{k}} + b})
$$

# 神经风格迁移

## 任务描述

------

什么是神经风格迁移？让我们来看几个例子，比如下图，左边的照片是在斯坦福大学的照片，右边的是梵高的星空，神经风格迁移可以帮你利用右边照片的风格来重新生成下面这张照片。

![](http://www.ai-start.com/dl2017/images/7b75c69ef064be274c82127a970461cf.png)

-   其中，$C$来表示内容图像。
-   $S$表示风格图像。
-   $G$表示生成图像。

## 代价函数

------

让我们为生成的图像定义一个代价函数，通过最小化代价函数，可以生成想要的任何图像，这个代价函数定义为两个部分：

1.   第一部分被称作内容代价函数，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片$G$的内容与内容图片$C$的内容有多相似。表示为$J_{\text{content}}(C,G)$
2.   第二部分是风格代价函数，也就是关于$S$和$G$的函数，用来度量图片$G$的风格和图片$S$的风格的相似度。表示为$J_{\text{style}}(S,G)$

最后用两个超参数$a$和$\beta$来来确定内容代价和风格代价：
$$
J( G) = a J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)
$$
算法的运行流程为：

1.   随机初始化生成图像$G$。（它可能像一堆雪花马赛克）
2.   然后使用代价函数$J(G)$，使用梯度下降的方法将其最小化，更新$G:= G - \frac{\partial}{\partial G}J(G)$

![](http://www.ai-start.com/dl2017/images/dd376e74155008845e96d662cc45493a.png)

### 内容代价函数

那么，如何定义内容代价$J_{\text{content}}(C,G)$？令$a^{[l][C]}$和$a^{[l][G]}$代表这两个图片$C$和$G$的$l$层的激活函数值。如果这两个激活值相似，那么就意味着两个图片的内容相似，所以：
$$
J_{\text{content}}( C,G) = \frac{1}{2}|| a^{[l][C]} - a^{[l][G]}||^{2}
$$

### 风格代价函数

那么，如何定义风格代价$J_{\text{style}}(S,G)$？什么又是**风格**？我们要做的就是将图片的**风格**定义为$l$层中激活项$\alpha$的各个通道之间相关系数。

对于$l$层的激活项$\alpha^{[l]}$，维度为$ n_{H} \times n_{W} \times n_{C}$，所以我们令$a_{i,\ j,\ k}^{[l]}$为隐藏层l中$(i,j,k)$位置的激活项，$i$，$j$，$k$分别代表该位置的高度、宽度以及对应的通道数。

对于$k$通道和$k'$通道之间的相关系数，公式为：
$$
G_{kk^{'}}^{[l]( S)} = \sum_{i = 1}^{n_{H}^{[l]}}{\sum_{j = 1}^{n_{W}^{[l]}}{a_{i,\ j,\ k}^{[l](S)}a_{i,\ j,\ k^{'}}^{[l](S)}}}
$$

$$
G_{kk^{'}}^{[l]( G)} = \sum_{i = 1}^{n_{H}^{[l]}}{\sum_{j = 1}^{n_{W}^{[l]}}{a_{i,\ j,\ k}^{[l](G)}a_{i,\ j,\ k^{'}}^{[l](G)}}}
$$

其中，上标$S$和$G$代表分别对**风格图像**和**生成图像**做计算。如果两个通道中的激活项数值都很大，那么$G_{{kk}^{'}}^{[l]}$也会变得很大，对应地，如果它们不相关那么$G_{{kk}^{'}}^{[l]}$就会很小。$G_{{kk}^{'}}^{[l]}$是一个数，它组成的$G^{[l]}$矩阵是个$n_{c} \times n_{c}$的矩阵，被称为**Gram**矩阵或风格矩阵。

最后组成风格代价函数：
$$
J_{\text{style}}^{[l]}(S,G)=\frac{1}{(2n_{H}^{[l]}n_{W}^{[l]}n_{C}^{[l]})^2}\Vert G^{[l]( S)}- G^{[l]( G)}\Vert_F^2
$$

$$
J_{\text{style}}(S,G)=\sum_l \lambda^{[l]}J_{\text{style}}^{[l]}(S,G)
$$

>   复习一下，下标$F$代表**Frobenius**范数，具体来说：
>   $$
>   \Vert G^{[l]( S)}- G^{[l]( G)}\Vert_F^2=\sum_k\sum_{k'}(G_{kk^{'}}^{[l]( S)}-G_{kk^{'}}^{[l]( G)})^2
>   $$

之后用梯度下降法，或者更复杂的优化算法来找到一个合适的图像$G$，并计算$J(G)$的最小值，最后能够得到非常漂亮的结果。

