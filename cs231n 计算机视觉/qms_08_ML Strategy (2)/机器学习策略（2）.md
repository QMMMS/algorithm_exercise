[TOC]

# 机器学习策略（2）

## 误差分析

------

误差分析通过统计不同错误标记类型占总数的百分比，可以帮助发现哪些问题需要优先解决，或者为构思新优化方向的灵感。一般的流程如下：

1.   找一组错误样本。
2.   统计属于不同错误类型的错误数量。
3.   在这个过程中，可能会得到启发，归纳出新的错误类型。
4.   确定错误占比最多的类别，专注它进行优化。

>   假设正在调试猫分类器，在错误例子中，会有狗的照片、狮子，豹，猎豹的照片，模糊图像，或者**Instagram**滤镜或**Snapchat**滤镜图片。其中模糊图像问题最严重，那么应该先解决模糊图像处理的问题。

## 清除标注错误的数据

------

监督学习问题的数据由输入$x$和输出标签 $y$ 构成，如果发现有些输出标签 $y$ 是错的是否值得花时间去修正这些标签呢？

![标注错误的数据](http://www.ai-start.com/dl2017/images/56f907e76f4fc8f589f1930128f77a98.png)

深度学习算法对随机误差很健壮，但对系统性的错误就没那么健壮了。所以比如说，如果做标记的人一直把白色的狗标记成猫，那就成问题了。因为分类器学习之后，会把所有白色的狗都分类为猫。但随机错误或近似随机错误，对于大多数深度学习算法来说不成问题。

建议是，如果这些标记错误严重影响了开发集上评估算法的能力，那么就应该去花时间修正错误的标签。但是，如果它们没有严重影响到开发集评估成本偏差的能力，那么可能就不应该花宝贵的时间去处理。类似上面提到的误差分析，统计错误占比最多的原因，优先对其优化。

现在如果决定要去修正开发集数据，手动重新检查标签，并尝试修正一些标签，这里还有一些额外的方针、原则和建议需要考虑：

-   修正手段要同时作用到开发集和测试集上，保持相同的分布。
-   要同时考虑检验算法判断正确和判断错误的样本。
-   修正训练集中的标签其实相对没那么重要，可能决定只修正开发集和测试集中的标签，这样其实是可以的。
-   在构造实际系统时，通常需要更多的人工错误分析，更多的人类见解来架构这些系统。
-   花时间亲自检查数据非常值得。（坐下来看100或几百个样本来统计错误数量）

## 快速搭建第一个系统，并进行迭代

------

一般来说，对于几乎所有的机器学习程序可能会有50个不同的方向可以前进，并且每个方向都是相对合理的可以改善系统。但挑战在于，如何选择一个方向集中精力处理。

所以建议，如果想搭建全新的机器学习程序，就是快速搭好第一个系统，然后开始迭代。

建立这个初始系统的所有意义在于，它可以是一个快速和粗糙的实现。初始系统的全部意义在于，有一个训练过的系统，确定偏差方差的范围，就可以知道下一步应该优先做什么，让能够进行错误分析，可以观察一些错误，然后想出所有能走的方向，哪些是实际上最有希望的方向。主要目标是弄出能用的系统，而不是发明全新的机器学习算法。

## 使用不同分布的数据进行训练和测试

------

在深度学习时代，越来越多的团队都用来自和开发集、测试集分布不同的数据来训练，让训练的数据量更大，里面有一些细节。

举个例子，假设你在开发一个手机应用，你想识别用户从应用中上传的图片是不是猫。你只收集到10,000张用户上传的照片，这些照片一般更业余，取景不太好，有些甚至很模糊。你从互联网上下载了超过20万张猫图，它们取景专业、高分辨率、拍摄专业。要用这些互联网图片吗？

显然这些互联网图片与用户上传的照片来自不同分布，互联网图片不是最好的选择。但是如果不用互联网图片，10,000张用户上传的照片太少了。

建议这样处理，训练集是来自网页下载的200,000张图片，如果需要的话，再加上5000张来自手机上传的图片。开发集就是2500张来自应用的图片，测试集也是2500张来自应用的图片。这样将数据分成训练集、开发集和测试集的好处在于，现在瞄准的目标就是想要处理的目标，开发集包含的数据全部来自手机上传，这是真正关心的图片分布。

## 数据分布不匹配时，偏差与方差的分析

------

>   复习一下，之前学习过：
>
>   -   当贝叶斯最优错误率与训练集错误率相差大，训练集错误率与开发集错误率相差小，在这种情况下，把重点应该放在减少偏差上。
>   -   当贝叶斯最优错误率与训练集错误率相差小，训练集错误率与开发集错误率相差大，在这种情况下，应该专注减少方差。

但如果训练数据和开发数据来自不同的分布，上面第二个结论不是完全正确的。

>   比如，也许算法在开发集上做得不错，可能因为训练集很容易识别，因为训练集都是高分辨率图片，很清晰的图像，但开发集要难以识别得多。所以也许软件没有方差问题，这只不过反映了开发集包含更难准确分类的图片。

为了弄清楚方差和数据分布不匹配哪个因素影响更大，需要定义一组新的数据，称之为训练-开发集。我们已经设立过这样的训练集、开发集和测试集，并且开发集和测试集来自相同的分布，但训练集来自不同的分布。我们随机打散训练集，然后分出一部分训练集作为训练-开发集，它只是为了进行误差分析，不要在它上面做训练。

-   假设训练误差为1%，训练-开发误差为1.5%，开发集错误率上升到10%。这是数据不匹配的问题。
-   假设训练误差是10%，训练-开发误差是11%，开发误差为12%，贝叶斯错误率的估计大概是0%，存在偏差问题。
-   假设训练误差是1%，训练-开发集上的误差是9%，开发集误差是10%，存在方差问题。
-   假设贝叶斯错误率是4%，训练错误率是7%，训练-开发错误率是10%。很意外，算法在开发集上做的更好，是6%。可能训练数据其实比开发集和测试集难识别得多。

## 处理数据不匹配问题

-------

如果训练集来自和开发测试集不同的分布，而且错误分析显示有一个数据不匹配的问题，该怎么办？这个问题没有完全系统的解决方案，但我们可以看看一些可以尝试的事情。

-   亲自做错误分析，尝试了解训练集和开发测试集的具体差异。
-   当认识到具体差异时，可以模拟开发集，人工合成数据添加到训练集。

举个例子，如果你正在开发一个语音激活的后视镜应用，你可能要听一下来自开发集的样本，尝试弄清楚开发集和训练集到底有什么不同。你可能会发现很多开发集样本噪音很多，有很多汽车噪音，这是你的开发集和训练集差异之一。

那么可以模拟车辆噪声数据，假设你录制了大量清晰的不带噪音的音频，你也可以收集一段汽车噪音，如果你把两个音频片段放到一起，你就可以合成出车背景噪音中的效果。通过人工数据合成，你可以快速制造更多的训练数据。

然而人工数据合成有一个潜在问题。比如说，你在安静的背景里录得10,000小时音频数据，然后，你只录了一小时车辆背景噪音，将这1小时汽车噪音回放10,000次，并叠加到在安静的背景下录得的10,000小时数据。有一个风险，有可能你的学习算法对这1小时汽车噪音过拟合。建议是要模拟全部数据空间，保证数据全面，与真实环境相似。

## 迁移学习

------

有的时候，神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中，这就是所谓的迁移学习。

>    例如，也许你已经训练好一个识别猫的神经网络，然后使用那些知识去训练阅读x射线扫描图的神经网络。

具体来说，要实现迁移学习，就是在之前训练好的基础上，把数据集换成新的$(x,y)$对，初始化最后一层的权重，称之为$w^{[L]}$和$b^{[L]}$随机初始化。

![](./img/tl.png)

经验规则是，如果你有一个小数据集，就只训练输出层前的最后一层，或者最后一两层。但是如果有很多数据，那么可以重新训练网络中的所有参数。如果重新训练神经网络中的所有参数，那么这个在图像识别数据的初期训练阶段，有时称为预训练（**pre-training**）。然后，如果以后更新所有权重，这个过程叫微调（**fine tuning**）。

>   Q：为什么迁移学习有效果呢？
>
>   A：还是上面把识别猫的神经网络训练阅读x射线扫描图的例子。从非常大的图像识别数据库中习得这些能力（边缘检测、曲线检测）可能有助于学习算法在放射科诊断中做得更好，算法学到了很多结构信息，图像形状的信息，这些知识有可能帮助放射科诊断网络学习更快一些，或者需要更少的学习数据。

迁移学习什么时候是有意义的呢？这里是使用迁移学习的典型场景：

-   迁移来源问题（旧问题）中你有很多数据，但迁移目标问题（新问题）你没有那么多数据。
-   迁移来源问题和迁移目标问题有相同的输入。
-   迁移来源问题和迁移目标问题要解决的底层特征细节相同。

## 多任务学习

------

在迁移学习中，步骤是串行的（从任务$A$里学习然后迁移到任务$B$）。在多任务学习中，是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。

举个例子，假设你在研发无人驾驶车辆，那么任务需要同时检测不同的物体，比如检测行人、车辆、停车标志，还有交通灯这四个物体。

![无人驾驶车辆要识别的图片](http://www.ai-start.com/dl2017/images/a4e496893ed0fb928300f59f26f89cf1.png)

如果这是输入图像$x^{(i)}$，那么这里不再是一个标签 $y^{(i)}$，而是有4个标签，在这个例子中$y^{(i)}$是个4×1向量，$Y = \begin{bmatrix}
| & | & | & \ldots & | \\
y^{(1)} & y^{(2)} & y^{(3)} & \ldots & y^{(m)} \\
| & | & | & \ldots & | \\
\end{bmatrix}$，要训练这个神经网络，需要改变最后的输出层结构，损失函数也要改变，公式为：
$$
J =\frac{1}{m}\sum_{i = 1}^{m}{\sum_{j = 1}^{4}{L(\hat y_{j}^{(i)},y_{j}^{(i)})}}
$$

$$
L(\hat y_{j}^{(i)},y_{j}^{(i)}) = - y_{j}^{(i)}\log\hat y_{j}^{(i)} - (1 - y_{j}^{(i)})log(1 - \hat y_{j}^{(i)})
$$

注意几个细节：

-   当然也可以训练四个不同的神经网络，而不是训练一个网络做四件事情。但神经网络一些早期特征，在识别不同物体时都会用到，然后你发现，训练一个神经网络做四件事情会比训练四个完全独立的神经网络分别做四件事性能要更好，这就是多任务学习的力量。
-   多任务学习也可以处理图像只有部分物体被标记的情况，标签不仅有0和1，还有问号表示未标记，也可以在上面训练算法，当有问号的时候，就在求和时忽略那个项。
-   在实践中，多任务学习的使用频率要低于迁移学习。但两者都可以成为强力工具。

多任务学习什么时候是有意义的呢？这里是使用多任务学习的典型场景：

-   训练的一组任务可以共用低层次特征。
-   其他任务加起来必须要有比单个任务大得多的数据量。
-   多任务学习会降低性能的唯一情况（和训练单个神经网络相比性能更低）就是神经网络还不够大。

## 端到端的深度学习

------

简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。

>   以语音识别为例，传统上，语音识别需要很多阶段的处理：首先提取一些特征，一些手工设计的音频特征，找到音位，串在一起构成独立的词，然后将词串起来构成听写文本。
>
>   端到端深度学习做的是，训练一个巨大的神经网络，输入就是一段音频，直接输出听写文本。

事实证明，端到端深度学习的挑战之一是可能需要大量数据才能让系统表现良好，并且它排除了可能有用的手工设计组件。当数据集较小的时候，传统流水线方法其实效果也不错，通常做得更好。
